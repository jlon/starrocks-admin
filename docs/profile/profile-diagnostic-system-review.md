# Query Profile è¯Šæ–­ç³»ç»Ÿæ·±åº¦å®¡æŸ¥ä¸æ”¹è¿›è®¾è®¡

> **ç‰ˆæœ¬**: v2.0  
> **æ—¥æœŸ**: 2024-12-07  
> **ä½œè€…**: StarRocks é«˜çº§æ¶æ„å¸ˆå®¡æŸ¥  
> **çŠ¶æ€**: å®¡æŸ¥å®Œæˆï¼Œå¾…å®æ–½æ”¹è¿›  
> **æ›´æ–°**: v2.0 - æ·±åº¦åæ€è§„åˆ™æŠ‘åˆ¶ã€é˜ˆå€¼åˆç†æ€§ã€å¤–è¡¨ç±»å‹ã€å†å²å¯¹æ¯”æŒä¹…åŒ–

---

## ä¸€ã€æ‰§è¡Œæ‘˜è¦

### 1.1 å½“å‰è¯„åˆ†ï¼š72/100

| ç»´åº¦ | æ»¡åˆ† | å¾—åˆ† | è¯´æ˜ |
|------|------|------|------|
| è§„åˆ™è¦†ç›–åº¦ | 25 | 20 | è¦†ç›–äº†ä¸»è¦ç®—å­ï¼Œä½†ç¼ºå°‘å…³é”®åœºæ™¯ |
| é˜ˆå€¼åˆç†æ€§ | 20 | 12 | ç¼ºä¹åŠ¨æ€é˜ˆå€¼ï¼Œç¡¬ç¼–ç è¿‡å¤š |
| æ™ºèƒ½åŒ–ç¨‹åº¦ | 20 | 10 | ç¼ºä¹ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œè§„åˆ™é—´ç¼ºä¹å…³è” |
| å»ºè®®å¯æ“ä½œæ€§ | 15 | 12 | å»ºè®®è¾ƒé€šç”¨ï¼Œç¼ºä¹é’ˆå¯¹æ€§ |
| å·¥ç¨‹å®ç° | 20 | 18 | ä»£ç ç»“æ„æ¸…æ™°ï¼Œä½†ç¼ºå°‘å…³é”®ä¿æŠ¤ |

### 1.2 æ ¸å¿ƒé—®é¢˜æ€»ç»“

1. **P0 ä¸¥é‡é—®é¢˜**ï¼šç¼ºä¹å…¨å±€æ‰§è¡Œæ—¶é—´é—¨æ§›ï¼Œæ¯«ç§’çº§æŸ¥è¯¢ä¹Ÿä¼šäº§ç”Ÿè¯Šæ–­
2. **P0 ä¸¥é‡é—®é¢˜**ï¼šè§„åˆ™ç¼ºä¹ç»å¯¹æ—¶é—´é—¨æ§›ï¼Œåªçœ‹å æ¯”ä¸çœ‹ç»å¯¹å€¼
3. **P1 é‡è¦é—®é¢˜**ï¼šç¼ºä¹æŸ¥è¯¢ç±»å‹æ„ŸçŸ¥ï¼ˆSELECT/INSERT/EXPORTï¼‰
4. **P1 é‡è¦é—®é¢˜**ï¼šè§„åˆ™é—´å…³ç³»è®¾è®¡ä¸å½“ï¼ˆç®€å•æŠ‘åˆ¶ä¼šä¸¢å¤±ä¿¡æ¯ï¼‰
5. **P2 æ”¹è¿›é¡¹**ï¼šé˜ˆå€¼ç¡¬ç¼–ç ï¼Œç¼ºå°‘åŠ¨æ€è°ƒæ•´
6. **P2 æ”¹è¿›é¡¹**ï¼šå¤–è¡¨ç±»å‹è¦†ç›–ä¸å…¨ï¼Œç¼ºå°‘ HDFS_SCAN ç­‰

---

## äºŒã€P0 ä¸¥é‡é—®é¢˜è¯¦ç»†åˆ†æ

### 2.1 é—®é¢˜ä¸€ï¼šç¼ºä¹å…¨å±€æ‰§è¡Œæ—¶é—´é—¨æ§›

**ç°çŠ¶åˆ†æ**ï¼š

å½“å‰ `RuleEngine::analyze_with_cluster_variables` æ–¹æ³•æ²¡æœ‰æ£€æŸ¥æŸ¥è¯¢æ€»æ‰§è¡Œæ—¶é—´ã€‚
å¯¹äºæ¯«ç§’çº§æŸ¥è¯¢ï¼ˆå¦‚ profile2 çš„ 11msï¼‰ï¼Œä»ç„¶ä¼šè§¦å‘ G001/G001b ç­‰è§„åˆ™ã€‚

**é—®é¢˜ç¤ºä¾‹**ï¼š
```
Profile2: æ€»æ‰§è¡Œæ—¶é—´ 11ms
- SCHEMA_SCAN: 50.75% â†’ è§¦å‘ G001bï¼ˆæ¬¡è€—æ—¶èŠ‚ç‚¹ï¼‰
- å®é™…åªæœ‰ 5.5msï¼Œæ ¹æœ¬ä¸éœ€è¦ä¼˜åŒ–
```

**ä¿®å¤æ–¹æ¡ˆ**ï¼š

```rust
// backend/src/services/profile_analyzer/analyzer/rule_engine.rs

/// å…¨å±€æ‰§è¡Œæ—¶é—´é—¨æ§›ï¼ˆæ¯«ç§’ï¼‰
const MIN_DIAGNOSIS_TIME_MS: f64 = 1000.0; // 1ç§’

pub fn analyze_with_cluster_variables(...) -> Vec<Diagnostic> {
    let total_time_ms = profile.summary.total_time_ms
        .or_else(|| parse_duration_ms(&profile.summary.total_time))
        .unwrap_or(0.0);
    
    if total_time_ms < MIN_DIAGNOSIS_TIME_MS {
        return vec![]; // å¿«é€ŸæŸ¥è¯¢ä¸éœ€è¦è¯Šæ–­
    }
    // ... åŸæœ‰é€»è¾‘
}
```

### 2.2 é—®é¢˜äºŒï¼šè§„åˆ™ç¼ºä¹ç»å¯¹æ—¶é—´é—¨æ§›

**ä¿®å¤æ–¹æ¡ˆ**ï¼š

```rust
// backend/src/services/profile_analyzer/analyzer/rules/common.rs

const MIN_OPERATOR_TIME_MS: f64 = 500.0; // 500ms

impl DiagnosticRule for G001MostConsuming {
    fn evaluate(&self, context: &RuleContext) -> Option<Diagnostic> {
        let percentage = context.get_time_percentage()?;
        let operator_time_ms = context.get_operator_time_ms()?;
        
        // åŒæ—¶æ£€æŸ¥å æ¯”å’Œç»å¯¹æ—¶é—´
        if percentage > 30.0 && operator_time_ms > MIN_OPERATOR_TIME_MS {
            Some(Diagnostic { ... })
        } else {
            None
        }
    }
}
```

---

## ä¸‰ã€è§„åˆ™å…³ç³»è®¾è®¡ï¼ˆv2.0 æ·±åº¦åæ€ï¼‰

### 3.1 åŸè®¾è®¡é—®é¢˜

åŸè®¾è®¡ä½¿ç”¨ç®€å•çš„"è§„åˆ™æŠ‘åˆ¶"ï¼š
```rust
// é”™è¯¯è®¾è®¡ï¼šS001 è§¦å‘åæŠ‘åˆ¶ S002
("S001", "S002", Suppresses)
```

**é—®é¢˜**ï¼šæ•°æ®å€¾æ–œï¼ˆS001ï¼‰å’Œ IO å€¾æ–œï¼ˆS002ï¼‰å¯èƒ½æ˜¯**ç‹¬ç«‹é—®é¢˜**ï¼š
- æ•°æ®å€¾æ–œï¼šæ•°æ®åˆ†å¸ƒä¸å‡ï¼ˆåˆ†æ¡¶é”®é—®é¢˜ï¼‰
- IO å€¾æ–œï¼šæŸäº›èŠ‚ç‚¹ç£ç›˜æ…¢ï¼ˆç¡¬ä»¶é—®é¢˜ï¼‰
- ä¸¤è€…å¯èƒ½åŒæ—¶å­˜åœ¨ï¼Œç®€å•æŠ‘åˆ¶ä¼šä¸¢å¤±é‡è¦ä¿¡æ¯

### 3.2 æ”¹è¿›è®¾è®¡ï¼šè§„åˆ™å…³ç³»ç±»å‹

```rust
/// è§„åˆ™å…³ç³»ç±»å‹
pub enum RuleRelation {
    /// äº’æ–¥ï¼šåŒä¸€æŒ‡æ ‡ä¸åŒé˜ˆå€¼ï¼Œåªä¿ç•™æ›´ä¸¥é‡çš„
    /// ä¾‹ï¼šG001(>30%) å’Œ G001b(>15%) äº’æ–¥
    MutuallyExclusive,
    
    /// å› æœï¼šA æ˜¯ B çš„æ ¹å› ï¼Œåˆå¹¶å±•ç¤ºå¹¶æ ‡æ³¨
    /// ä¾‹ï¼šS001 æ•°æ®å€¾æ–œ â†’ G003 æ‰§è¡Œæ—¶é—´å€¾æ–œ
    Causal { root_cause: &'static str },
    
    /// ç‹¬ç«‹ï¼šå¯åŒæ—¶å­˜åœ¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    /// ä¾‹ï¼šS001 æ•°æ®å€¾æ–œ å’Œ S002 IOå€¾æ–œ
    Independent,
}

/// è§„åˆ™å…³ç³»é…ç½®
pub const RULE_RELATIONS: &[(&str, &str, RuleRelation)] = &[
    // äº’æ–¥å…³ç³»
    ("G001", "G001b", RuleRelation::MutuallyExclusive),
    
    // å› æœå…³ç³»ï¼šæ•°æ®å€¾æ–œå¯¼è‡´æ‰§è¡Œæ—¶é—´å€¾æ–œ
    ("S001", "G003", RuleRelation::Causal { root_cause: "S001" }),
    
    // ç‹¬ç«‹å…³ç³»ï¼šæ•°æ®å€¾æ–œå’Œ IO å€¾æ–œæ˜¯ä¸åŒé—®é¢˜
    ("S001", "S002", RuleRelation::Independent),
    
    // å› æœå…³ç³»ï¼šJoin ç»“æœè†¨èƒ€å¯¼è‡´å†…å­˜è¿‡é«˜
    ("J001", "G002", RuleRelation::Causal { root_cause: "J001" }),
];
```

### 3.3 å¤„ç†é€»è¾‘

```rust
impl RuleEngine {
    fn process_relations(&self, diagnostics: Vec<Diagnostic>) -> Vec<Diagnostic> {
        let mut result = Vec::new();
        let mut processed = HashSet::new();
        
        for diag in &diagnostics {
            if processed.contains(&diag.rule_id) {
                continue;
            }
            
            // æŸ¥æ‰¾ç›¸å…³è§„åˆ™
            for (rule_a, rule_b, relation) in RULE_RELATIONS {
                match relation {
                    RuleRelation::MutuallyExclusive => {
                        // åªä¿ç•™æ›´ä¸¥é‡çš„
                        processed.insert(rule_b);
                    }
                    RuleRelation::Causal { root_cause } => {
                        // åˆå¹¶å±•ç¤ºï¼Œæ ‡æ³¨æ ¹å› 
                        if &diag.rule_id == root_cause {
                            // åœ¨å»ºè®®ä¸­æ ‡æ³¨è¿™æ˜¯æ ¹å› 
                            diag.message = format!("ğŸ” æ ¹å› : {}", diag.message);
                        }
                    }
                    RuleRelation::Independent => {
                        // éƒ½ä¿ç•™ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
                    }
                }
            }
            result.push(diag.clone());
        }
        result
    }
}
```

---

## å››ã€é˜ˆå€¼åˆç†æ€§æ·±åº¦åæ€ï¼ˆv2.0 æ›´æ–°ï¼‰

### 4.1 å½“å‰é˜ˆå€¼é—®é¢˜æ±‡æ€»

| é˜ˆå€¼ | å½“å‰å€¼ | é—®é¢˜ | å»ºè®®å€¼ |
|------|--------|------|--------|
| å…¨å±€æ‰§è¡Œæ—¶é—´é—¨æ§› | **æ— ** | ä¸¥é‡ç¼ºå¤± | 1sï¼ˆOLAPï¼‰/ åŠ¨æ€ï¼ˆETLï¼‰ |
| G001 æ—¶é—´å æ¯” | 30% | âœ… åˆç† | ä¿æŒï¼ˆå¯¹é½ StarRocksï¼‰ |
| G002 å†…å­˜ | 1GB | **å¤ªç»å¯¹** | BE å†…å­˜çš„ 10% |
| S001 æ•°æ®å€¾æ–œ | max/avg > 2 | **å¯èƒ½å¤ªä¸¥æ ¼** | 2.5-3.0ï¼ˆè€ƒè™‘å¹¶è¡Œåº¦ï¼‰ |
| S009 ç¼“å­˜å‘½ä¸­ | < 30% | **å¤ªä¸¥æ ¼** | < 50% |
| Q001 æ‰§è¡Œæ—¶é—´ | 60s | **å¤ªå®½æ¾** | OLAP 10s / ETL 5min |
| å°æ–‡ä»¶å¹³å‡å¤§å° | 10MB | **å¤ªä¸¥æ ¼** | 64MBï¼ˆHDFSï¼‰/ 128MBï¼ˆS3ï¼‰ |

### 4.2 åŠ¨æ€é˜ˆå€¼è®¾è®¡


```rust
/// åŠ¨æ€é˜ˆå€¼è®¡ç®—å™¨
pub struct DynamicThresholds {
    cluster_info: ClusterInfo,
    query_type: QueryType,
}

impl DynamicThresholds {
    /// å†…å­˜é˜ˆå€¼ï¼šç›¸å¯¹äº BE å†…å­˜é…ç½®
    pub fn get_memory_threshold(&self) -> u64 {
        let be_memory = self.cluster_info.be_memory_limit
            .unwrap_or(64 * 1024 * 1024 * 1024); // é»˜è®¤ 64GB
        (be_memory as f64 * 0.1) as u64 // å•ç®—å­ä¸è¶…è¿‡ BE å†…å­˜çš„ 10%
    }
    
    /// æ‰§è¡Œæ—¶é—´é˜ˆå€¼ï¼šæ ¹æ®æŸ¥è¯¢ç±»å‹
    pub fn get_time_threshold(&self) -> f64 {
        match self.query_type {
            QueryType::Select => 10_000.0,     // OLAP: 10s
            QueryType::Insert => 300_000.0,    // ETL: 5min
            QueryType::Export => 600_000.0,    // Export: 10min
            QueryType::Analyze => 600_000.0,   // Analyze: 10min
            QueryType::Load => 1800_000.0,     // Load: 30min
            _ => 60_000.0,                     // é»˜è®¤: 1min
        }
    }
    
    /// æ•°æ®å€¾æ–œé˜ˆå€¼ï¼šæ ¹æ®å¹¶è¡Œåº¦åŠ¨æ€è°ƒæ•´
    pub fn get_skew_threshold(&self) -> f64 {
        let parallelism = self.cluster_info.backend_num;
        match parallelism {
            p if p > 32 => 3.5,  // å¤§é›†ç¾¤å…è®¸æ›´å¤§å€¾æ–œ
            p if p > 16 => 3.0,
            p if p > 8 => 2.5,
            _ => 2.0,           // å°é›†ç¾¤æ›´ä¸¥æ ¼
        }
    }
    
    /// å°æ–‡ä»¶é˜ˆå€¼ï¼šæ ¹æ®å­˜å‚¨ç±»å‹
    pub fn get_small_file_threshold(&self, storage_type: &str) -> u64 {
        match storage_type {
            "S3" | "OSS" | "COS" | "GCS" => 128 * 1024 * 1024,  // å¯¹è±¡å­˜å‚¨: 128MB
            "HDFS" => 64 * 1024 * 1024,                         // HDFS: 64MB (å—å¤§å°)
            "LOCAL" => 32 * 1024 * 1024,                        // æœ¬åœ°: 32MB
            _ => 64 * 1024 * 1024,                              // é»˜è®¤: 64MB
        }
    }
    
    /// ç¼“å­˜å‘½ä¸­ç‡é˜ˆå€¼ï¼šæ ¹æ®å­˜å‚¨ç±»å‹
    pub fn get_cache_hit_threshold(&self, is_disaggregated: bool) -> f64 {
        if is_disaggregated {
            0.5  // å­˜ç®—åˆ†ç¦»ï¼š50% å‘½ä¸­ç‡æ˜¯è­¦å‘Šçº¿
        } else {
            0.3  // å…±äº«å­˜å‚¨ï¼š30% å‘½ä¸­ç‡æ˜¯è­¦å‘Šçº¿
        }
    }
}
```

### 4.3 é˜ˆå€¼é…ç½®æ–‡ä»¶

```yaml
# config/diagnostic_thresholds.yaml
global:
  # å…¨å±€æ‰§è¡Œæ—¶é—´é—¨æ§›ï¼ˆæ¯«ç§’ï¼‰
  min_diagnosis_time_ms: 1000
  # ç®—å­ç»å¯¹æ—¶é—´é—¨æ§›ï¼ˆæ¯«ç§’ï¼‰
  min_operator_time_ms: 500

time_percentage:
  most_consuming: 30.0      # å¯¹é½ StarRocks isMostConsuming
  second_most_consuming: 15.0  # å¯¹é½ StarRocks isSecondMostConsuming

data_skew:
  # æ ¹æ®å¹¶è¡Œåº¦åŠ¨æ€è°ƒæ•´
  base_ratio: 2.0
  parallelism_factor: 0.05  # æ¯å¢åŠ  10 å¹¶è¡Œåº¦ï¼Œé˜ˆå€¼ +0.5

memory:
  # ç›¸å¯¹é˜ˆå€¼ï¼ˆBE å†…å­˜ç™¾åˆ†æ¯”ï¼‰
  operator_peak_percent: 10
  hash_table_percent: 5
  # ç»å¯¹é˜ˆå€¼ï¼ˆå…œåº•ï¼‰
  operator_peak_max: 10737418240  # 10GB
  hash_table_max: 5368709120      # 5GB

small_files:
  # æŒ‰å­˜å‚¨ç±»å‹é…ç½®
  s3:
    min_file_count: 500
    min_avg_size: 134217728  # 128MB
  hdfs:
    min_file_count: 500
    min_avg_size: 67108864   # 64MB
  local:
    min_file_count: 200
    min_avg_size: 33554432   # 32MB

cache:
  # å­˜ç®—åˆ†ç¦»åœºæ™¯
  disaggregated_hit_rate: 0.5
  # å…±äº«å­˜å‚¨åœºæ™¯
  shared_storage_hit_rate: 0.3

cardinality:
  error_ratio: 10.0  # å®é™…/ä¼°ç®— > 10 å€
```

---

## äº”ã€å¤–è¡¨ç±»å‹å®Œæ•´è¦†ç›–ï¼ˆv2.0 æ›´æ–°ï¼‰

### 5.1 å½“å‰å®ç°ç¼ºå¤±

```rust
// å½“å‰å®ç°ç¼ºå°‘å¤šç§å¤–è¡¨ç±»å‹
fn applicable_to(&self, node: &ExecutionTreeNode) -> bool {
    let name = node.operator_name.to_uppercase();
    name.contains("CONNECTOR_SCAN") || 
    name.contains("HIVE_SCAN") || 
    name.contains("ICEBERG_SCAN") ||
    name.contains("HUDI_SCAN") ||
    name.contains("DELTALAKE_SCAN")
    // ç¼ºå°‘: HDFS_SCAN, FILE_SCAN, PAIMON_SCAN, JDBC_SCAN ç­‰
}
```

### 5.2 å®Œæ•´çš„å¤–è¡¨ Scan ç±»å‹

```rust
/// å¤–è¡¨ Scan ç±»å‹æšä¸¾
pub enum ExternalScanType {
    // æ•°æ®æ¹–æ ¼å¼
    Hive,
    Iceberg,
    Hudi,
    DeltaLake,
    Paimon,
    
    // æ–‡ä»¶ç³»ç»Ÿ
    Hdfs,
    File,
    S3,
    
    // å¤–éƒ¨æ•°æ®åº“
    Jdbc,
    Mysql,
    Elasticsearch,
    
    // é€šç”¨è¿æ¥å™¨
    Connector,
    
    // æœªçŸ¥
    Unknown,
}

impl ExternalScanType {
    pub fn from_operator_name(name: &str) -> Option<Self> {
        let upper = name.to_uppercase();
        
        if upper.contains("HIVE_SCAN") { return Some(Self::Hive); }
        if upper.contains("ICEBERG_SCAN") { return Some(Self::Iceberg); }
        if upper.contains("HUDI_SCAN") { return Some(Self::Hudi); }
        if upper.contains("DELTALAKE_SCAN") { return Some(Self::DeltaLake); }
        if upper.contains("PAIMON_SCAN") { return Some(Self::Paimon); }
        if upper.contains("HDFS_SCAN") { return Some(Self::Hdfs); }
        if upper.contains("FILE_SCAN") { return Some(Self::File); }
        if upper.contains("S3_SCAN") { return Some(Self::S3); }
        if upper.contains("JDBC_SCAN") { return Some(Self::Jdbc); }
        if upper.contains("MYSQL_SCAN") { return Some(Self::Mysql); }
        if upper.contains("ES_SCAN") { return Some(Self::Elasticsearch); }
        if upper.contains("CONNECTOR_SCAN") { return Some(Self::Connector); }
        
        None
    }
    
    /// æ˜¯å¦é€‚ç”¨å°æ–‡ä»¶æ£€æµ‹
    pub fn supports_small_file_detection(&self) -> bool {
        matches!(self, 
            Self::Hive | Self::Iceberg | Self::Hudi | 
            Self::DeltaLake | Self::Paimon | Self::Hdfs | 
            Self::File | Self::S3 | Self::Connector
        )
    }
    
    /// è·å–å­˜å‚¨ç±»å‹ï¼ˆç”¨äºé˜ˆå€¼è®¡ç®—ï¼‰
    pub fn storage_type(&self) -> &'static str {
        match self {
            Self::S3 => "S3",
            Self::Hdfs | Self::Hive => "HDFS",
            Self::Iceberg | Self::Hudi | Self::DeltaLake | Self::Paimon => "HDFS", // é€šå¸¸åŸºäº HDFS
            Self::File => "LOCAL",
            _ => "UNKNOWN",
        }
    }
    
    /// è·å–å°æ–‡ä»¶æ£€æµ‹çš„æŒ‡æ ‡å
    pub fn file_count_metric(&self) -> &'static str {
        match self {
            Self::Hdfs => "BlocksRead",
            _ => "ScanRanges",
        }
    }
}
```

### 5.3 æ›´æ–°åçš„å°æ–‡ä»¶æ£€æµ‹è§„åˆ™

```rust
/// S016: å¤–è¡¨å°æ–‡ä»¶æ£€æµ‹ï¼ˆv2.0 æ›´æ–°ï¼‰
pub struct S016ExternalSmallFiles;

impl DiagnosticRule for S016ExternalSmallFiles {
    fn id(&self) -> &str { "S016" }
    fn name(&self) -> &str { "å¤–è¡¨å°æ–‡ä»¶è¿‡å¤š" }

    fn applicable_to(&self, node: &ExecutionTreeNode) -> bool {
        ExternalScanType::from_operator_name(&node.operator_name)
            .map(|t| t.supports_small_file_detection())
            .unwrap_or(false)
    }

    fn evaluate(&self, context: &RuleContext) -> Option<Diagnostic> {
        let scan_type = ExternalScanType::from_operator_name(
            &context.node.operator_name
        )?;
        
        // è·å–æ–‡ä»¶æ•°é‡æŒ‡æ ‡
        let metric_name = scan_type.file_count_metric();
        let file_count = context.get_metric(metric_name)
            .or_else(|| context.get_metric("MorselsCount"))?;
        
        let bytes_read = context.get_metric("BytesRead").unwrap_or(0.0);
        
        if file_count < 100.0 { return None; }
        
        let avg_file_size = if file_count > 0.0 { 
            bytes_read / file_count 
        } else { 
            0.0 
        };
        
        // æ ¹æ®å­˜å‚¨ç±»å‹è·å–é˜ˆå€¼
        let storage_type = scan_type.storage_type();
        let threshold = context.get_small_file_threshold(storage_type);
        
        if file_count > 500.0 && avg_file_size < threshold as f64 {
            let table_name = context.node.unique_metrics
                .get("Table")
                .map(|s| s.as_str())
                .unwrap_or("external_table");
            
            // æ ¹æ®å¤–è¡¨ç±»å‹ç”Ÿæˆé’ˆå¯¹æ€§å»ºè®®
            let suggestions = generate_small_file_suggestions(&scan_type, table_name);
            
            Some(Diagnostic {
                rule_id: self.id().to_string(),
                rule_name: self.name().to_string(),
                severity: RuleSeverity::Warning,
                node_path: format!("{} (plan_node_id={})", 
                    context.node.operator_name,
                    context.node.plan_node_id.unwrap_or(-1)),
                plan_node_id: context.node.plan_node_id,
                message: format!(
                    "æ‰«æäº† {:.0} ä¸ªæ–‡ä»¶ï¼Œå¹³å‡å¤§å°ä»… {}ï¼ˆå»ºè®® > {}ï¼‰",
                    file_count, 
                    format_bytes(avg_file_size as u64),
                    format_bytes(threshold)
                ),
                reason: format!(
                    "å¤–è¡¨ {} å­˜åœ¨å¤§é‡å°æ–‡ä»¶ï¼Œå¯¼è‡´å…ƒæ•°æ®å¼€é”€å¤§ã€IO æ•ˆç‡ä½ã€‚",
                    table_name
                ),
                suggestions,
                parameter_suggestions: vec![],
            })
        } else {
            None
        }
    }
}

fn generate_small_file_suggestions(scan_type: &ExternalScanType, table: &str) -> Vec<String> {
    match scan_type {
        ExternalScanType::Hive => vec![
            format!("åˆå¹¶å°æ–‡ä»¶: INSERT OVERWRITE {} SELECT * FROM {}", table, table),
            "è°ƒæ•´ Hive è¡¨çš„ mapreduce.input.fileinputformat.split.minsize".to_string(),
        ],
        ExternalScanType::Iceberg => vec![
            format!("æ‰§è¡Œ Compaction: CALL rewrite_data_files(table => '{}')", table),
            "è°ƒæ•´ write.target-file-size-bytes å‚æ•°".to_string(),
        ],
        ExternalScanType::Hudi => vec![
            "æ‰§è¡Œ Hudi Compaction åˆå¹¶å°æ–‡ä»¶".to_string(),
            "è°ƒæ•´ hoodie.parquet.small.file.limit å‚æ•°".to_string(),
        ],
        ExternalScanType::DeltaLake => vec![
            format!("æ‰§è¡Œ OPTIMIZE {} ZORDER BY ...", table),
            "å¯ç”¨ Auto Compaction".to_string(),
        ],
        ExternalScanType::Hdfs => vec![
            "ä½¿ç”¨ Hadoop Archive (HAR) åˆå¹¶å°æ–‡ä»¶".to_string(),
            "è°ƒæ•´ä¸Šæ¸¸ ETL è¾“å‡ºæ–‡ä»¶å¤§å°ï¼ˆå»ºè®® 128MB-256MBï¼‰".to_string(),
        ],
        _ => vec![
            "åˆå¹¶å°æ–‡ä»¶ä»¥æå‡æŸ¥è¯¢æ€§èƒ½".to_string(),
            "è€ƒè™‘å°†çƒ­ç‚¹æ•°æ®å¯¼å…¥ StarRocks å†…è¡¨".to_string(),
        ],
    }
}
```

---

## å…­ã€æŸ¥è¯¢æŒ‡çº¹ä¸å†å²å¯¹æ¯”ï¼ˆv2.0 æ·±åº¦è®¾è®¡ï¼‰

### 6.1 æ˜¯å¦éœ€è¦æŒä¹…åŒ–ï¼Ÿ

| åœºæ™¯ | æ˜¯å¦éœ€è¦æŒä¹…åŒ– | å­˜å‚¨æ–¹æ¡ˆ |
|------|--------------|---------|
| å•æ¬¡è¯Šæ–­ | âŒ ä¸éœ€è¦ | - |
| ä¼šè¯å†…å¯¹æ¯” | âŒ ä¸éœ€è¦ | å†…å­˜ç¼“å­˜ |
| è·¨ä¼šè¯å¯¹æ¯” | âœ… éœ€è¦ | æœ¬åœ° SQLite |
| å¤šç”¨æˆ·å…±äº« | âœ… éœ€è¦ | è¿œç¨‹å­˜å‚¨ |
| ç”Ÿäº§ç›‘æ§ | âœ… éœ€è¦ | è¿œç¨‹å­˜å‚¨ + å‘Šè­¦ |

**å»ºè®®åˆ†é˜¶æ®µå®æ–½**ï¼š
- **MVP**ï¼šåªç”¨å†…å­˜ç¼“å­˜ï¼Œä¸æŒä¹…åŒ–
- **V1**ï¼šæœ¬åœ° SQLite æŒä¹…åŒ–
- **V2**ï¼šå¯é€‰è¿œç¨‹å­˜å‚¨ï¼ˆå¦‚ StarRocks è‡ªèº«ï¼‰

### 6.2 å­˜å‚¨æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æŸ¥è¯¢å†å²å­˜å‚¨æ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  å†…å­˜ç¼“å­˜    â”‚    â”‚  æœ¬åœ°å­˜å‚¨    â”‚    â”‚  è¿œç¨‹å­˜å‚¨    â”‚     â”‚
â”‚  â”‚  (LRU)      â”‚â”€â”€â”€â–¶â”‚  (SQLite)   â”‚â”€â”€â”€â–¶â”‚  (å¯é€‰)     â”‚     â”‚
â”‚  â”‚  10K æ¡ç›®   â”‚    â”‚  30 å¤©ä¿ç•™   â”‚    â”‚  StarRocks  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                  â”‚                  â”‚             â”‚
â”‚         â–¼                  â–¼                  â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                QueryHistoryService                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 æ•°æ®ç»“æ„è®¾è®¡

```rust
/// æŸ¥è¯¢æŒ‡çº¹ï¼ˆç”¨äºè¯†åˆ«ç›¸ä¼¼æŸ¥è¯¢ï¼‰
#[derive(Hash, Eq, PartialEq, Clone)]
pub struct QueryFingerprint {
    /// SQL æ¨¡æ¿ï¼ˆå‚æ•°åŒ–åï¼‰
    /// "SELECT * FROM t WHERE id = 123" â†’ "SELECT * FROM t WHERE id = ?"
    pub sql_template: String,
    /// æ¶‰åŠçš„è¡¨ï¼ˆæ’åºåï¼‰
    pub tables: Vec<String>,
    /// æŸ¥è¯¢ç±»å‹
    pub query_type: QueryType,
}

impl QueryFingerprint {
    pub fn from_profile(profile: &Profile) -> Self {
        let sql = profile.summary.sql.as_deref().unwrap_or("");
        Self {
            sql_template: Self::normalize_sql(sql),
            tables: Self::extract_tables(sql),
            query_type: QueryType::from_profile(&profile.summary),
        }
    }
    
    /// SQL å‚æ•°åŒ–ï¼šå°†å…·ä½“å€¼æ›¿æ¢ä¸ºå ä½ç¬¦
    fn normalize_sql(sql: &str) -> String {
        // ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„ SQL è§£æ
        let mut result = sql.to_string();
        // æ›¿æ¢æ•°å­—
        result = regex::Regex::new(r"\b\d+\b").unwrap()
            .replace_all(&result, "?").to_string();
        // æ›¿æ¢å­—ç¬¦ä¸²
        result = regex::Regex::new(r"'[^']*'").unwrap()
            .replace_all(&result, "?").to_string();
        result
    }
}

/// æ‰§è¡ŒåŸºçº¿ï¼ˆèšåˆç»Ÿè®¡ï¼‰
pub struct ExecutionBaseline {
    pub fingerprint_hash: u64,
    pub sample_count: u32,
    pub time_stats: TimeStats,
    pub resource_stats: ResourceStats,
    pub last_updated: DateTime<Utc>,
}

pub struct TimeStats {
    pub p50_ms: f64,
    pub p90_ms: f64,
    pub p99_ms: f64,
    pub avg_ms: f64,
}

pub struct ResourceStats {
    pub avg_memory_bytes: u64,
    pub avg_scan_bytes: u64,
    pub avg_shuffle_bytes: u64,
}
```

### 6.4 å­˜å‚¨ç­–ç•¥é…ç½®

```rust
pub struct HistoryConfig {
    /// å†…å­˜ç¼“å­˜å¤§å°ï¼ˆæŒ‡çº¹æ•°é‡ï¼‰
    pub memory_cache_size: usize,      // é»˜è®¤ 10000
    
    /// æœ¬åœ°å­˜å‚¨ä¿ç•™å¤©æ•°
    pub local_retention_days: u32,     // é»˜è®¤ 30
    
    /// æ˜¯å¦å¯ç”¨è¿œç¨‹å­˜å‚¨
    pub enable_remote_storage: bool,   // é»˜è®¤ false
    
    /// é‡‡æ ·ç‡ï¼ˆé¿å…å­˜å‚¨è¿‡å¤šï¼‰
    pub sampling_rate: f64,            // é»˜è®¤ 0.1 (10%)
    
    /// æœ€å°æ‰§è¡Œæ—¶é—´ï¼ˆå¤ªå¿«çš„ä¸è®°å½•ï¼‰
    pub min_record_time_ms: f64,       // é»˜è®¤ 100ms
    
    /// æœ€å°æ ·æœ¬æ•°ï¼ˆæ ·æœ¬å¤ªå°‘ä¸åˆ¤æ–­å›å½’ï¼‰
    pub min_samples_for_regression: u32, // é»˜è®¤ 10
}

impl Default for HistoryConfig {
    fn default() -> Self {
        Self {
            memory_cache_size: 10000,
            local_retention_days: 30,
            enable_remote_storage: false,
            sampling_rate: 0.1,
            min_record_time_ms: 100.0,
            min_samples_for_regression: 10,
        }
    }
}
```

### 6.5 å›å½’æ£€æµ‹é€»è¾‘

```rust
impl QueryHistoryService {
    /// æ£€æµ‹æ€§èƒ½å›å½’
    pub fn detect_regression(
        &self,
        fingerprint: &QueryFingerprint,
        current_time_ms: f64,
    ) -> Option<RegressionDiagnostic> {
        let baseline = self.get_baseline(fingerprint)?;
        
        // æ ·æœ¬å¤ªå°‘ä¸åˆ¤æ–­
        if baseline.sample_count < self.config.min_samples_for_regression {
            return None;
        }
        
        // è®¡ç®—å›å½’æ¯”ç‡ï¼ˆä¸ P90 å¯¹æ¯”ï¼‰
        let ratio = current_time_ms / baseline.time_stats.p90_ms;
        
        if ratio > 2.0 {
            Some(RegressionDiagnostic {
                rule_id: "REG001".to_string(),
                rule_name: "æ€§èƒ½å›å½’".to_string(),
                severity: if ratio > 5.0 { 
                    RuleSeverity::Error 
                } else { 
                    RuleSeverity::Warning 
                },
                message: format!(
                    "æŸ¥è¯¢æ‰§è¡Œæ—¶é—´ {:.1}msï¼Œæ˜¯å†å² P90ï¼ˆ{:.1}msï¼‰çš„ {:.1} å€",
                    current_time_ms, baseline.time_stats.p90_ms, ratio
                ),
                baseline_p90_ms: baseline.time_stats.p90_ms,
                current_ms: current_time_ms,
                regression_ratio: ratio,
                sample_count: baseline.sample_count,
            })
        } else {
            None
        }
    }
    
    /// è®°å½•æ‰§è¡Œï¼ˆé‡‡æ ·ï¼‰
    pub fn record_execution(&self, fingerprint: &QueryFingerprint, metrics: &ExecutionMetrics) {
        // é‡‡æ ·æ§åˆ¶
        if rand::random::<f64>() > self.config.sampling_rate {
            return;
        }
        
        // å¤ªå¿«çš„ä¸è®°å½•
        if metrics.total_time_ms < self.config.min_record_time_ms {
            return;
        }
        
        // æ›´æ–°åŸºçº¿
        self.update_baseline(fingerprint, metrics);
    }
}
```

---

## ä¸ƒã€æŸ¥è¯¢ç±»å‹æ„ŸçŸ¥


### 7.1 æŸ¥è¯¢ç±»å‹å®šä¹‰

```rust
/// æŸ¥è¯¢ç±»å‹
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum QueryType {
    Select,     // æ™®é€šæŸ¥è¯¢
    Insert,     // INSERT INTO SELECT
    Export,     // EXPORT å¯¼å‡º
    Analyze,    // ANALYZE TABLE
    Ctas,       // CREATE TABLE AS SELECT
    Load,       // Broker Load / Routine Load
    Unknown,
}

impl QueryType {
    pub fn from_profile(summary: &ProfileSummary) -> Self {
        let sql = summary.sql.as_deref().unwrap_or("").to_uppercase();
        let sql = sql.trim();
        
        if sql.starts_with("INSERT") {
            QueryType::Insert
        } else if sql.starts_with("EXPORT") {
            QueryType::Export
        } else if sql.starts_with("ANALYZE") {
            QueryType::Analyze
        } else if sql.starts_with("CREATE TABLE") && sql.contains("AS SELECT") {
            QueryType::Ctas
        } else if sql.starts_with("LOAD") || sql.contains("BROKER LOAD") {
            QueryType::Load
        } else if sql.starts_with("SELECT") {
            QueryType::Select
        } else {
            QueryType::Unknown
        }
    }
}
```

### 7.2 æŸ¥è¯¢ç±»å‹ç‰¹å®šé…ç½®

```rust
impl QueryType {
    /// è·å–æ‰§è¡Œæ—¶é—´é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
    pub fn get_time_threshold(&self) -> f64 {
        match self {
            QueryType::Select => 10_000.0,     // OLAP: 10s
            QueryType::Insert => 300_000.0,    // ETL: 5min
            QueryType::Export => 600_000.0,    // Export: 10min
            QueryType::Analyze => 600_000.0,   // Analyze: 10min
            QueryType::Ctas => 300_000.0,      // CTAS: 5min
            QueryType::Load => 1800_000.0,     // Load: 30min
            QueryType::Unknown => 60_000.0,    // é»˜è®¤: 1min
        }
    }
    
    /// è·å–é€‚ç”¨çš„è§„åˆ™é›†
    pub fn applicable_rules(&self) -> Vec<&'static str> {
        match self {
            QueryType::Select => vec![
                "G001", "G001b", "G002", "G003",  // é€šç”¨è§„åˆ™
                "S001", "S003", "S007", "S009",   // Scan è§„åˆ™
                "J001", "J002", "J004",           // Join è§„åˆ™
                "Q001", "Q002", "Q005",           // Query è§„åˆ™
            ],
            QueryType::Insert | QueryType::Ctas => vec![
                "G001", "G002", "G003",           // é€šç”¨è§„åˆ™ï¼ˆä¸å« G001bï¼‰
                "S001", "S007",                   // Scan è§„åˆ™ï¼ˆä¸å«è¿‡æ»¤æ•ˆæœï¼‰
                "I001", "I002", "I003",           // Sink è§„åˆ™
                // ä¸åŒ…å« Q001ï¼ˆæ‰§è¡Œæ—¶é—´é˜ˆå€¼ä¸åŒï¼‰
            ],
            QueryType::Export => vec![
                "G002", "G003",                   // å†…å­˜å’Œå€¾æ–œ
                // IO å æ¯”é«˜æ˜¯æ­£å¸¸çš„ï¼Œä¸æ£€æµ‹
            ],
            QueryType::Analyze => vec![
                "G002",                           // åªæ£€æµ‹å†…å­˜
                // æ‰«æé‡å¤§æ˜¯æ­£å¸¸çš„
            ],
            QueryType::Load => vec![
                "I001", "I002", "I003",           // åªæ£€æµ‹å¯¼å…¥ç›¸å…³
            ],
            QueryType::Unknown => vec![
                "G001", "G001b", "G002", "G003",  // æ‰€æœ‰é€šç”¨è§„åˆ™
            ],
        }
    }
    
    /// æ˜¯å¦åº”è¯¥è·³è¿‡æŸä¸ªè§„åˆ™
    pub fn should_skip_rule(&self, rule_id: &str) -> bool {
        !self.applicable_rules().contains(&rule_id)
    }
}
```

---

## å…«ã€æ–°å¢è§„åˆ™è®¾è®¡

### 8.1 å°æ–‡ä»¶æ£€æµ‹è§„åˆ™ï¼ˆS015/S016ï¼‰

è§ç¬¬äº”èŠ‚çš„å®Œæ•´å®ç°ã€‚

### 8.2 ç»Ÿè®¡ä¿¡æ¯è§„åˆ™ï¼ˆSTAT001/STAT002ï¼‰

```rust
/// STAT001: åŸºæ•°ä¼°ç®—åå·®å¤§
pub struct STAT001CardinalityError;

impl DiagnosticRule for STAT001CardinalityError {
    fn id(&self) -> &str { "STAT001" }
    fn name(&self) -> &str { "åŸºæ•°ä¼°ç®—åå·®å¤§" }

    fn applicable_to(&self, _node: &ExecutionTreeNode) -> bool {
        true
    }

    fn evaluate(&self, context: &RuleContext) -> Option<Diagnostic> {
        let estimated = context.get_metric("EstimatedRows")
            .or_else(|| context.get_metric("Cardinality"))?;
        let actual = context.node.rows.unwrap_or(0) as f64;
        
        if estimated <= 0.0 || actual <= 0.0 { return None; }
        
        let ratio = (actual / estimated).max(estimated / actual);
        
        if ratio > 10.0 {
            let table_name = context.node.unique_metrics
                .get("Table")
                .map(|s| s.as_str())
                .unwrap_or("unknown");
            
            Some(Diagnostic {
                rule_id: self.id().to_string(),
                rule_name: self.name().to_string(),
                severity: RuleSeverity::Warning,
                node_path: format!("{} (plan_node_id={})", 
                    context.node.operator_name,
                    context.node.plan_node_id.unwrap_or(-1)),
                plan_node_id: context.node.plan_node_id,
                message: format!(
                    "åŸºæ•°ä¼°ç®—åå·® {:.1} å€ï¼ˆå®é™… {:.0} è¡Œï¼Œä¼°ç®— {:.0} è¡Œï¼‰",
                    ratio, actual, estimated
                ),
                reason: "ä¼˜åŒ–å™¨åŸºæ•°ä¼°ç®—ä¸å®é™…æ‰§è¡Œç»“æœåå·®è¿‡å¤§ï¼Œå¯èƒ½å¯¼è‡´æ‰§è¡Œè®¡åˆ’ä¸ä¼˜ã€‚".to_string(),
                suggestions: vec![
                    format!("æ‰§è¡Œ ANALYZE TABLE {}; æ›´æ–°ç»Ÿè®¡ä¿¡æ¯", table_name),
                    "æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯æ”¶é›†æ—¶é—´: SHOW STATS META".to_string(),
                ],
                parameter_suggestions: vec![],
            })
        } else {
            None
        }
    }
}
```

### 8.3 åˆ†åŒºè£å‰ªè§„åˆ™ï¼ˆPART001ï¼‰

```rust
/// PART001: åˆ†åŒºè£å‰ªæœªç”Ÿæ•ˆ
pub struct PART001PartitionPruning;

impl DiagnosticRule for PART001PartitionPruning {
    fn id(&self) -> &str { "PART001" }
    fn name(&self) -> &str { "åˆ†åŒºè£å‰ªæœªç”Ÿæ•ˆ" }

    fn applicable_to(&self, node: &ExecutionTreeNode) -> bool {
        node.operator_name.to_uppercase().contains("SCAN")
    }

    fn evaluate(&self, context: &RuleContext) -> Option<Diagnostic> {
        let scanned = context.get_metric("PartitionsScanned")?;
        let total = context.get_metric("TotalPartitions")?;
        
        if total < 10.0 { return None; } // åˆ†åŒºå¤ªå°‘ä¸æ£€æµ‹
        
        let ratio = scanned / total;
        
        if ratio > 0.5 {
            let table_name = context.node.unique_metrics
                .get("Table")
                .map(|s| s.as_str())
                .unwrap_or("unknown");
            
            Some(Diagnostic {
                rule_id: self.id().to_string(),
                rule_name: self.name().to_string(),
                severity: RuleSeverity::Warning,
                node_path: format!("{} (plan_node_id={})", 
                    context.node.operator_name,
                    context.node.plan_node_id.unwrap_or(-1)),
                plan_node_id: context.node.plan_node_id,
                message: format!(
                    "æ‰«æäº† {:.0}/{:.0} ä¸ªåˆ†åŒº ({:.1}%)",
                    scanned, total, ratio * 100.0
                ),
                reason: "åˆ†åŒºè£å‰ªæœªèƒ½æœ‰æ•ˆå‡å°‘æ‰«æèŒƒå›´ï¼Œå¯èƒ½æ˜¯ WHERE æ¡ä»¶æœªåŒ…å«åˆ†åŒºé”®ã€‚".to_string(),
                suggestions: vec![
                    "æ£€æŸ¥ WHERE æ¡ä»¶æ˜¯å¦åŒ…å«åˆ†åŒºé”®".to_string(),
                    "æ£€æŸ¥åˆ†åŒºé”®ç±»å‹æ˜¯å¦åŒ¹é…ï¼ˆé¿å…éšå¼è½¬æ¢ï¼‰".to_string(),
                    format!("æŸ¥çœ‹åˆ†åŒºä¿¡æ¯: SHOW PARTITIONS FROM {}", table_name),
                ],
                parameter_suggestions: vec![],
            })
        } else {
            None
        }
    }
}
```

---

## ä¹ã€å•å…ƒæµ‹è¯•æ”¹è¿›

### 9.1 å…³é”®æµ‹è¯•ç”¨ä¾‹

```rust
#[cfg(test)]
mod tests {
    use super::*;

    /// P0: å¿«é€ŸæŸ¥è¯¢ä¸åº”äº§ç”Ÿè¯Šæ–­
    #[test]
    fn test_fast_query_no_diagnostics() {
        let profile = create_test_profile_with_time("11ms");
        let engine = RuleEngine::new();
        let diagnostics = engine.analyze(&profile);
        
        assert!(diagnostics.is_empty(),
            "å¿«é€ŸæŸ¥è¯¢ï¼ˆ11msï¼‰ä¸åº”äº§ç”Ÿè¯Šæ–­ï¼Œä½†å¾—åˆ°äº† {} æ¡", diagnostics.len());
    }

    /// P0: ç®—å­ç»å¯¹æ—¶é—´é—¨æ§›
    #[test]
    fn test_operator_absolute_time_threshold() {
        let profile = create_test_profile_with_operator(100.0, 50.0); // 100ms æŸ¥è¯¢ï¼Œç®—å­å  50%
        let engine = RuleEngine::new();
        let diagnostics = engine.analyze(&profile);
        
        let g001 = diagnostics.iter().find(|d| d.rule_id == "G001");
        assert!(g001.is_none(), "50ms çš„ç®—å­ä¸åº”è§¦å‘ G001");
    }

    /// è§„åˆ™å…³ç³»ï¼šäº’æ–¥
    #[test]
    fn test_rule_mutual_exclusion() {
        let profile = create_test_profile_with_high_percentage(35.0);
        let engine = RuleEngine::new();
        let diagnostics = engine.analyze(&profile);
        
        let g001 = diagnostics.iter().filter(|d| d.rule_id == "G001").count();
        let g001b = diagnostics.iter().filter(|d| d.rule_id == "G001b").count();
        
        assert!(g001 > 0, "G001 åº”è¯¥è§¦å‘");
        assert_eq!(g001b, 0, "G001b åº”è¯¥è¢« G001 äº’æ–¥");
    }

    /// è§„åˆ™å…³ç³»ï¼šç‹¬ç«‹ï¼ˆS001 å’Œ S002 å¯åŒæ—¶å­˜åœ¨ï¼‰
    #[test]
    fn test_rule_independence() {
        let profile = create_test_profile_with_both_skews();
        let engine = RuleEngine::new();
        let diagnostics = engine.analyze(&profile);
        
        let s001 = diagnostics.iter().any(|d| d.rule_id == "S001");
        let s002 = diagnostics.iter().any(|d| d.rule_id == "S002");
        
        // ä¸¤è€…å¯ä»¥åŒæ—¶å­˜åœ¨
        assert!(s001 || s002, "è‡³å°‘åº”è§¦å‘ä¸€ä¸ªå€¾æ–œè§„åˆ™");
    }

    /// æŸ¥è¯¢ç±»å‹æ„ŸçŸ¥
    #[test]
    fn test_query_type_awareness() {
        let profile = create_test_profile_with_sql("INSERT INTO t1 SELECT * FROM t2");
        let query_type = QueryType::from_profile(&profile.summary);
        
        assert_eq!(query_type, QueryType::Insert);
        assert_eq!(query_type.get_time_threshold(), 300_000.0);
        assert!(query_type.should_skip_rule("Q001")); // INSERT ä¸æ£€æµ‹ Q001
    }

    /// å¤–è¡¨å°æ–‡ä»¶æ£€æµ‹
    #[test]
    fn test_external_small_files() {
        let node = create_test_node("HDFS_SCAN", vec![
            ("BlocksRead", "2000"),
            ("BytesRead", "1073741824"), // 1GB / 2000 = 512KB avg
        ]);
        
        let rule = S016ExternalSmallFiles;
        let context = create_test_context(&node);
        let result = rule.evaluate(&context);
        
        assert!(result.is_some(), "åº”æ£€æµ‹åˆ° HDFS å°æ–‡ä»¶é—®é¢˜");
    }

    /// åŠ¨æ€é˜ˆå€¼ï¼šå†…å­˜
    #[test]
    fn test_dynamic_memory_threshold() {
        let cluster_info = ClusterInfo {
            backend_num: 8,
            be_memory_limit: Some(128 * 1024 * 1024 * 1024), // 128GB
            ..Default::default()
        };
        
        let thresholds = DynamicThresholds::new(cluster_info, QueryType::Select);
        let memory_threshold = thresholds.get_memory_threshold();
        
        // 128GB * 10% = 12.8GB
        assert_eq!(memory_threshold, 12 * 1024 * 1024 * 1024 + 800 * 1024 * 1024);
    }

    /// åŠ¨æ€é˜ˆå€¼ï¼šæ•°æ®å€¾æ–œ
    #[test]
    fn test_dynamic_skew_threshold() {
        let small_cluster = ClusterInfo { backend_num: 4, ..Default::default() };
        let large_cluster = ClusterInfo { backend_num: 64, ..Default::default() };
        
        let small_threshold = DynamicThresholds::new(small_cluster, QueryType::Select)
            .get_skew_threshold();
        let large_threshold = DynamicThresholds::new(large_cluster, QueryType::Select)
            .get_skew_threshold();
        
        assert!(large_threshold > small_threshold, 
            "å¤§é›†ç¾¤åº”å…è®¸æ›´å¤§çš„å€¾æ–œé˜ˆå€¼");
    }
}
```

---

## åã€å®æ–½è®¡åˆ’ï¼ˆæ›´æ–°ï¼‰

### 10.1 ä¼˜å…ˆçº§æ’åº

| ä¼˜å…ˆçº§ | æ”¹è¿›é¡¹ | é¢„ä¼°å·¥ä½œé‡ | æ”¶ç›Š |
|--------|--------|-----------|------|
| **P0** | å…¨å±€æ‰§è¡Œæ—¶é—´é—¨æ§› | 0.5å¤© | é¿å…è¯¯æŠ¥ |
| **P0** | è§„åˆ™ç»å¯¹æ—¶é—´é—¨æ§› | 1å¤© | é¿å…è¯¯æŠ¥ |
| **P0** | æ–°å¢å•å…ƒæµ‹è¯• | 1å¤© | è´¨é‡ä¿è¯ |
| **P1** | æŸ¥è¯¢ç±»å‹æ„ŸçŸ¥ | 1å¤© | å‡å°‘å™ªéŸ³ |
| **P1** | è§„åˆ™å…³ç³»é‡æ„ï¼ˆäº’æ–¥/å› æœ/ç‹¬ç«‹ï¼‰ | 2å¤© | æå‡å‡†ç¡®æ€§ |
| **P1** | å¤–è¡¨ç±»å‹å®Œå–„ï¼ˆHDFS_SCAN ç­‰ï¼‰ | 0.5å¤© | è¦†ç›–å®Œæ•´ |
| **P1** | å°æ–‡ä»¶æ£€æµ‹è§„åˆ™ | 1å¤© | è¦†ç›–å…³é”®åœºæ™¯ |
| **P2** | åŠ¨æ€é˜ˆå€¼å®ç° | 2å¤© | æ›´æ™ºèƒ½ |
| **P2** | ç»Ÿè®¡ä¿¡æ¯è§„åˆ™ | 1å¤© | è¦†ç›–å…³é”®åœºæ™¯ |
| **P3** | å†å²å¯¹æ¯”ï¼ˆå†…å­˜ç¼“å­˜ï¼‰ | 2å¤© | åŸºç¡€åŠŸèƒ½ |
| **P3** | å†å²å¯¹æ¯”ï¼ˆSQLite æŒä¹…åŒ–ï¼‰ | 3å¤© | å®Œæ•´åŠŸèƒ½ |

### 10.2 åˆ†é˜¶æ®µå®æ–½

**ç¬¬ä¸€é˜¶æ®µï¼ˆP0ï¼Œ2.5 å¤©ï¼‰**ï¼š
1. å…¨å±€æ‰§è¡Œæ—¶é—´é—¨æ§›
2. ç®—å­ç»å¯¹æ—¶é—´é—¨æ§›
3. å…³é”®å•å…ƒæµ‹è¯•

**ç¬¬äºŒé˜¶æ®µï¼ˆP1ï¼Œ5.5 å¤©ï¼‰**ï¼š
1. æŸ¥è¯¢ç±»å‹æ„ŸçŸ¥
2. è§„åˆ™å…³ç³»é‡æ„
3. å¤–è¡¨ç±»å‹å®Œå–„
4. å°æ–‡ä»¶æ£€æµ‹è§„åˆ™

**ç¬¬ä¸‰é˜¶æ®µï¼ˆP2ï¼Œ3 å¤©ï¼‰**ï¼š
1. åŠ¨æ€é˜ˆå€¼å®ç°
2. ç»Ÿè®¡ä¿¡æ¯è§„åˆ™

**ç¬¬å››é˜¶æ®µï¼ˆP3ï¼Œ5 å¤©ï¼‰**ï¼š
1. å†å²å¯¹æ¯”ï¼ˆå†…å­˜ç¼“å­˜ï¼‰
2. å†å²å¯¹æ¯”ï¼ˆSQLite æŒä¹…åŒ–ï¼‰

---

## åä¸€ã€å®Œæ•´è§„åˆ™æ¸…å•

| è§„åˆ™ID | åç§° | ç±»å‹ | çŠ¶æ€ | å¤‡æ³¨ |
|--------|------|------|------|------|
| G001 | ç®—å­æ—¶é—´å æ¯”è¿‡é«˜ | é€šç”¨ | âœ… éœ€ä¿®æ”¹ | æ·»åŠ ç»å¯¹æ—¶é—´é—¨æ§› |
| G001b | ç®—å­æ—¶é—´å æ¯”è¾ƒé«˜ | é€šç”¨ | âœ… éœ€ä¿®æ”¹ | æ·»åŠ ç»å¯¹æ—¶é—´é—¨æ§› |
| G002 | ç®—å­å†…å­˜ä½¿ç”¨è¿‡é«˜ | é€šç”¨ | âœ… éœ€ä¿®æ”¹ | æ”¹ä¸ºåŠ¨æ€é˜ˆå€¼ |
| G003 | ç®—å­æ‰§è¡Œæ—¶é—´å€¾æ–œ | é€šç”¨ | âœ… éœ€ä¿®æ”¹ | åŠ¨æ€å€¾æ–œé˜ˆå€¼ |
| S001-S014 | Scan è§„åˆ™ | Scan | âœ… å·²å®ç° | - |
| **S015** | å†…è¡¨ Rowset ç¢ç‰‡åŒ– | Scan | ğŸ†• å¾…å®ç° | - |
| **S016** | å¤–è¡¨å°æ–‡ä»¶è¿‡å¤š | Scan | ğŸ†• å¾…å®ç° | æ”¯æŒ HDFS_SCAN |
| J001-J010 | Join è§„åˆ™ | Join | âœ… å·²å®ç° | - |
| A001-A005 | Aggregate è§„åˆ™ | Aggregate | âœ… å·²å®ç° | - |
| T001-T005 | Sort è§„åˆ™ | Sort | âœ… å·²å®ç° | - |
| W001 | çª—å£åˆ†åŒºè¿‡å®½ | Window | âœ… å·²å®ç° | - |
| E001-E003 | Exchange è§„åˆ™ | Exchange | âœ… å·²å®ç° | - |
| Q001-Q009 | Query è§„åˆ™ | Query | âœ… éœ€ä¿®æ”¹ | æŸ¥è¯¢ç±»å‹æ„ŸçŸ¥ |
| I001-I003 | Sink è§„åˆ™ | Sink | âœ… å·²å®ç° | - |
| P001 | Project è¡¨è¾¾å¼è®¡ç®—æ…¢ | Project | âœ… å·²å®ç° | - |
| L001 | LocalExchange å†…å­˜è¿‡é«˜ | LocalExchange | âœ… å·²å®ç° | - |
| **STAT001** | åŸºæ•°ä¼°ç®—åå·®å¤§ | é€šç”¨ | ğŸ†• å¾…å®ç° | - |
| **STAT002** | ç»Ÿè®¡ä¿¡æ¯ç¼ºå¤± | Scan | ğŸ†• å¾…å®ç° | - |
| **PART001** | åˆ†åŒºè£å‰ªæœªç”Ÿæ•ˆ | Scan | ğŸ†• å¾…å®ç° | - |
| **COL001** | å¯ä½¿ç”¨ Colocate Join | Join | ğŸ†• å¾…å®ç° | - |
| **REG001** | æ€§èƒ½å›å½’ | Query | ğŸ†• å¾…å®ç° | éœ€è¦å†å²å¯¹æ¯” |

---

## åäºŒã€æ€»ç»“

### 12.1 å…³é”®æ”¹è¿›ç‚¹

1. **è§„åˆ™æŠ‘åˆ¶ â†’ è§„åˆ™å…³ç³»**ï¼šä»ç®€å•æŠ‘åˆ¶æ”¹ä¸ºäº’æ–¥/å› æœ/ç‹¬ç«‹ä¸‰ç§å…³ç³»ï¼Œé¿å…ä¸¢å¤±ä¿¡æ¯
2. **å›ºå®šé˜ˆå€¼ â†’ åŠ¨æ€é˜ˆå€¼**ï¼šå†…å­˜ã€å€¾æ–œã€å°æ–‡ä»¶é˜ˆå€¼æ ¹æ®é›†ç¾¤é…ç½®å’Œå­˜å‚¨ç±»å‹åŠ¨æ€è°ƒæ•´
3. **å¤–è¡¨ç±»å‹å®Œå–„**ï¼šè¡¥å…… HDFS_SCANã€PAIMON_SCAN ç­‰ï¼Œå¹¶é’ˆå¯¹ä¸åŒç±»å‹ç”Ÿæˆé’ˆå¯¹æ€§å»ºè®®
4. **å†å²å¯¹æ¯”åˆ†å±‚**ï¼šMVP ç”¨å†…å­˜ç¼“å­˜ï¼Œåç»­æŒ‰éœ€æ·»åŠ  SQLite æŒä¹…åŒ–

### 12.2 é¢„æœŸæ•ˆæœ

é€šè¿‡å®æ–½ä»¥ä¸Šæ”¹è¿›ï¼Œé¢„è®¡ï¼š
- è¯¯æŠ¥ç‡é™ä½ 80%ï¼ˆå…¨å±€æ—¶é—´é—¨æ§› + ç»å¯¹æ—¶é—´é—¨æ§›ï¼‰
- è¯Šæ–­å‡†ç¡®æ€§æå‡ 30%ï¼ˆè§„åˆ™å…³ç³»é‡æ„ + åŠ¨æ€é˜ˆå€¼ï¼‰
- è¦†ç›–åœºæ™¯å¢åŠ  20%ï¼ˆæ–°å¢è§„åˆ™ + å¤–è¡¨ç±»å‹å®Œå–„ï¼‰
- ç³»ç»Ÿè¯„åˆ†ä» 72 åˆ†æå‡è‡³ **90+ åˆ†**
